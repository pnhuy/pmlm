{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAxIWg0olGUF"
   },
   "source": [
    "# Tutorial 3a - Medical text analysis\n",
    "\n",
    "Original author: [Santeri Rytky](https://www.oulu.fi/fi/tutkijat/santeri-rytky) \n",
    "\n",
    "Natural language processing (NLP) is a specific subset of artificial intelligence, covering all techniques that are used to process linguistic and textual data. Practical examples include speech recognition, text-to-speech conversion, automatic grammar correction and junk email detection. In this tutorial, you will use an open-source dataset to classify medical abstracts between 5 different conditions. Topics covered:\n",
    "\n",
    "- Basics of natural language processing\n",
    "- Multiclass classification\n",
    "- Pipelining multiple analysis steps\n",
    "- Accounting for dataset bias\n",
    "- Objective selection of model parameters (Hyperparameter optimization)\n",
    "\n",
    "Original dataset:\n",
    "https://www.kaggle.com/chaitanyakck/medical-text\n",
    "\n",
    "References/more reading:\n",
    "https://realpython.com/python-keras-text-classification/\n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18741,
     "status": "ok",
     "timestamp": 1621316246018,
     "user": {
      "displayName": "Santeri Rytky",
      "photoUrl": "",
      "userId": "00637237915246853555"
     },
     "user_tz": -180
    },
    "id": "RhJ3yYOqlHqc",
    "outputId": "e5805e0d-3acb-41b6-d22f-0b3b031c1341"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Mount the Google drive folder to Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Add the Tutorial folder to Python path\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/MLinMedicine/Tutorial2')\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNAyXSNLTFSE"
   },
   "source": [
    "Remember that you can press `ctrl` and hover over the module to see more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11542,
     "status": "ok",
     "timestamp": 1621938721852,
     "user": {
      "displayName": "Santeri Rytky",
      "photoUrl": "",
      "userId": "00637237915246853555"
     },
     "user_tz": -180
    },
    "id": "32PWGJs3Qs3C",
    "outputId": "79fee71f-1094-40ff-f16f-3fa5b9437a7e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Timing and dates\n",
    "import time\n",
    "\n",
    "# Wrap long texts\n",
    "from textwrap import fill\n",
    "\n",
    "# Assign default variables to functions\n",
    "from functools import partial\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "# Combine analysis steps\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# NLP functions\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9au4m2L6GKPr"
   },
   "source": [
    "# 1. Data loading and preprocessing\n",
    "Let's start by loading the data into the notebook environment. If you open the .dat file, you can see that it consists of numbers followed by a couple of sentences. The number represents the condition of the patient, and the sentences consist of the medical abstract. The number and the abstract are separated by a tabulator, `\\t`. The collection of texts is often called a **corpus** in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_s9B4F-iN1Nx"
   },
   "outputs": [],
   "source": [
    "# Read the abstracts data and the corresponding labels\n",
    "with open('./train.dat', 'r') as f:\n",
    "    text_data = f.readlines() \n",
    "\n",
    "# Split labels and text\n",
    "corpus, labels = [], []\n",
    "for line in text_data:\n",
    "    # Split data and label based on the tab. Split the abstract into words.\n",
    "    corpus.append(line.split('\\t')[1])\n",
    "    labels.append(int(line.split('\\t')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrWJ4O0CYLf8"
   },
   "source": [
    "Data has been divided to 5 classes:\n",
    "\n",
    "- 1 = Neoplasms (tumor)\n",
    "- 2 = Digestive system diseases\n",
    "- 3 = Nervous system diseases\n",
    "- 4 = Cardiovascular diseases\n",
    "- 5 = General pathological conditions\n",
    "\n",
    "In contrast to tutorial 2, we now need to identify multiple diseases, classes (**multi-class classification**), instead of the patient death (**binary classification**). Let's verify the number of abstracts and classes. It is also a good idea to read a couple of abstracts to get a sense of their contents in this dataset. You can change the value for `id` to get another set of abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1621320478500,
     "user": {
      "displayName": "Santeri Rytky",
      "photoUrl": "",
      "userId": "00637237915246853555"
     },
     "user_tz": -180
    },
    "id": "xEoMmBibFohH",
    "outputId": "8f65a253-889a-4daa-9aae-330c130b15c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset consists of 14438 medical texts from 5 classes\n",
      "Abstracts for \n",
      " class 1: 3163\n",
      " class 2: 1494\n",
      " class 3: 1925\n",
      " class 4: 3051\n",
      " class 5: 4805\n",
      "\n",
      "Class 1:\n",
      "\n",
      " Modified anterior compartment resection. In the majority of patients\n",
      "with soft tissue sarcomas of th\n",
      "\n",
      "Class 2:\n",
      "\n",
      " Postoperative pancreatic abscess due to Plesiomonas shigelloides.\n",
      "Plesiomonas shigelloides is being\n",
      "\n",
      "Class 3:\n",
      "\n",
      " Meralgia paresthetica after coronary bypass surgery. Meralgia\n",
      "paresthetica is a neurologic disorder\n",
      "\n",
      "Class 4:\n",
      "\n",
      " A rheolytic system for percutaneous coronary and peripheral plaque\n",
      "removal. A method for plaque diss\n",
      "\n",
      "Class 5:\n",
      "\n",
      " A controlled trial comparing vidarabine with acyclovir in neonatal\n",
      "herpes simplex virus infection. I\n"
     ]
    }
   ],
   "source": [
    "print(f'The dataset consists of {len(corpus)} medical texts from {len(np.unique(labels))} classes')\n",
    "\n",
    "# Are the classes balanced?\n",
    "labels_array = np.array(labels)\n",
    "np.sum(labels_array == 4)\n",
    "print(f'Abstracts for \\n class 1: {np.sum(labels_array == 1)}\\n',\n",
    "f'class 2: {np.sum(labels_array == 2)}\\n',\n",
    "f'class 3: {np.sum(labels_array == 3)}\\n',\n",
    "f'class 4: {np.sum(labels_array == 4)}\\n',\n",
    "f'class 5: {np.sum(labels_array == 5)}\\n')\n",
    "\n",
    "# Example abstracts\n",
    "id = 7\n",
    "print('Class 1:\\n\\n', fill(corpus[np.where(labels_array == 1)[0][id]][:100]))\n",
    "print('\\nClass 2:\\n\\n', fill(corpus[np.where(labels_array == 2)[0][id]][:100]))\n",
    "print('\\nClass 3:\\n\\n', fill(corpus[np.where(labels_array == 3)[0][id]][:100]))\n",
    "print('\\nClass 4:\\n\\n', fill(corpus[np.where(labels_array == 4)[0][id]][:100]))\n",
    "print('\\nClass 5:\\n\\n', fill(corpus[np.where(labels_array == 5)[0][id]][:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eq65aNmObZ6R"
   },
   "source": [
    "Note that the classes 2 and 3 are slightly less frequent in the data. Next, we can split the data into training, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13920,
     "status": "ok",
     "timestamp": 1621316264656,
     "user": {
      "displayName": "Santeri Rytky",
      "photoUrl": "",
      "userId": "00637237915246853555"
     },
     "user_tz": -180
    },
    "id": "Rp3lY1AgG5nb",
    "outputId": "972867c7-0ee2-4e86-b55f-44e2d4664421"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set consists of 6480 abstracts, while the validation set includes 3193 abstracts\n",
      "Test set includes 4765 abstracts\n"
     ]
    }
   ],
   "source": [
    "# Fix random seed\n",
    "seed = 2\n",
    "\n",
    "# Train and test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(corpus, labels, test_size=0.33, random_state=seed, shuffle=True)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.33, random_state=seed, shuffle=True)\n",
    "print(f'Training set consists of {len(X_train)} abstracts, while the validation set includes {len(X_val)} abstracts')\n",
    "print(f'Test set includes {len(X_test)} abstracts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsDgyPSIS2qx"
   },
   "source": [
    "# 2. Baseline\n",
    "\n",
    "Let's start by making a simple baseline experiment. A baseline refers to a simple reference method that we can later try to improve. The results of the baseline experiment also describe how difficult the given task is. \n",
    "\n",
    "A conventional machine learning approach to classify textual data is to first to convert the text into a feature vector by counting the occurrences of each word. The features can then be used to create a machine learning model. This is known as a **bag-of-words** approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOk6Dt_rLNhU"
   },
   "source": [
    "Next, we will do the **feature extraction**. In the simple case, we can count occurrences for each word, and assign an index for each unique word. This means that we can represent the abstract as a vector. **Feature vectors** can then easily be fed to the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16381,
     "status": "ok",
     "timestamp": 1621316267903,
     "user": {
      "displayName": "Santeri Rytky",
      "photoUrl": "",
      "userId": "00637237915246853555"
     },
     "user_tz": -180
    },
    "id": "mbXLrnohKnSO",
    "outputId": "7bddd2af-3da7-4d2a-9042-ceaeac349c0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words used in first abstract:\n",
      " ['21' '24' 'activation' 'activity' 'acute' 'administered' 'administration'\n",
      " 'all' 'almost' 'amidase' 'an' 'and' 'animals' 'antiprotease'\n",
      " 'antiproteases' 'approach' 'are' 'as' 'at' 'balance' 'be' 'been'\n",
      " 'benefit' 'but' 'by' 'capacities' 'capacity' 'complete' 'confirmed'\n",
      " 'consumption' 'continues' 'current' 'data' 'defences' 'degree' 'during'\n",
      " 'dying' 'enzyme' 'enzymes' 'examined' 'exogenous' 'experimental'\n",
      " 'exudate' 'exudates' 'factor' 'few' 'for' 'formed' 'from' 'fulminant'\n",
      " 'has' 'human' 'in' 'indicating' 'inhibitory' 'instances'\n",
      " 'intraperitoneal' 'intraperitoneally' 'intravenously' 'is' 'key' 'likely'\n",
      " 'man' 'marked' 'may' 'not' 'occur' 'of' 'or' 'other' 'overwhelming'\n",
      " 'pancreas' 'pancreatitis' 'patients' 'peritoneal' 'possessed' 'prolongs'\n",
      " 'protease' 'proteolytic' 'rats' 'reduced' 'reduction' 'release'\n",
      " 'released' 'responsible' 'role' 'sampling' 'shocked' 'showed' 'study'\n",
      " 'such' 'suggested' 'suggests' 'survival' 'that' 'the' 'their' 'therapy'\n",
      " 'this' 'three' 'time' 'to' 'toxic' 'toxicity' 'trypsin' 'trypsinogen'\n",
      " 'tryptic' 'two' 'was' 'were' 'when' 'where' 'with' 'within' 'zymogen']\n",
      "Feature vector type: <class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction:\n",
    "# Count the word occurrences. \n",
    "# Note that the words are converted to lowercase by default\n",
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "\n",
    "# .fit() creates the vocabulary based on X_train\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "# .transform() converts texts into feature vectors based on the fitted vocabulary\n",
    "X_train_vector = vectorizer.transform(X_train)\n",
    "X_val_vector = vectorizer.transform(X_val)\n",
    "X_test_vector = vectorizer.transform(X_test)\n",
    "\n",
    "# Which words occur in the first abstract?\n",
    "words = np.nonzero(X_train_vector[0, :])[1]\n",
    "f_names = vectorizer.get_feature_names_out()\n",
    "print('Words used in first abstract:\\n', np.take(f_names, words))\n",
    "print(f'Feature vector type: {type(X_train_vector)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAbGHr7rPHvn"
   },
   "source": [
    "Note that the vectorizer sorts the words alphabetically. We have now created a **vocabulary** including all words in the training set. The feature vectors are sparse matrices, which are optimized for having only a couple of nonzero elements (one sample does not include all words of the vocabulary).\n",
    "\n",
    "Next, we can train the classifier based on the word occurrences. Let's use a simple logistic regression model for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70149,
     "status": "ok",
     "timestamp": 1621316322231,
     "user": {
      "displayName": "Santeri Rytky",
      "photoUrl": "",
      "userId": "00637237915246853555"
     },
     "user_tz": -180
    },
    "id": "SPVy-DnOMOLE",
    "outputId": "781cdd8b-a89d-48b1-e90e-633bdb26ed14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.495\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.61      0.61       705\n",
      "           2       0.42      0.36      0.39       338\n",
      "           3       0.43      0.36      0.39       427\n",
      "           4       0.57      0.54      0.56       639\n",
      "           5       0.42      0.49      0.45      1084\n",
      "\n",
      "    accuracy                           0.49      3193\n",
      "   macro avg       0.49      0.47      0.48      3193\n",
      "weighted avg       0.50      0.49      0.49      3193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier for 1000 iterations\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_vector, Y_train)\n",
    "\n",
    "# Predict on validation data\n",
    "predictions = model.predict(X_val_vector)\n",
    "\n",
    "# Predict on test data for later\n",
    "predictions_bline = model.predict(X_test_vector)\n",
    "\n",
    "# Show average accuracy with 3 decimal places\n",
    "print(f'Accuracy: {accuracy_score(predictions, Y_val):.3f}')\n",
    "# Print a report for multiclass prediction performance\n",
    "print(classification_report(Y_val, predictions, labels=[1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFnhN6uiPlpN"
   },
   "source": [
    "The classification report provides a lot of useful information:\n",
    "\n",
    "- Precision = true positives / total amount of predicted positives\n",
    "- Recall = true positives / total amount of positives\n",
    "- F1 score = weighted score accounting both precision and recall\n",
    "- Support = number of cases for the class\n",
    "- Accuracy = correct predictions / incorrect predictions\n",
    "- macro average = unweighted average across the classes\n",
    "- weighted average = weighted according to support values\n",
    "\n",
    "# 3. Improving the performance: stop words\n",
    "\n",
    "What if we remove the stopwords from the vocabulary? These are commonly used words that occur in the english language and do not add much value for classification, such as: *the, and, is, in*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89983,
     "status": "ok",
     "timestamp": 1621316342593,
     "user": {
      "displayName": "Santeri Rytky",
      "photoUrl": "",
      "userId": "00637237915246853555"
     },
     "user_tz": -180
    },
    "id": "_0omPr__QGbv",
    "outputId": "f9a998b1-196e-4e67-d251-da734dc2c6e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.497\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.61      0.62       705\n",
      "           2       0.43      0.37      0.39       338\n",
      "           3       0.43      0.37      0.40       427\n",
      "           4       0.58      0.56      0.57       639\n",
      "           5       0.42      0.48      0.45      1084\n",
      "\n",
      "    accuracy                           0.50      3193\n",
      "   macro avg       0.50      0.48      0.49      3193\n",
      "weighted avg       0.50      0.50      0.50      3193\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Count the word occurrences removing the stopwords \n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "vectorizer.fit(X_train)\n",
    "X_train_vector = vectorizer.transform(X_train)\n",
    "X_val_vector = vectorizer.transform(X_val)\n",
    "\n",
    "# Train the classifier\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vector, Y_train)\n",
    "\n",
    "# Predict on validation data\n",
    "predictions = model.predict(X_val_vector)\n",
    "print(f'Accuracy: {accuracy_score(predictions, Y_val):.3f}')\n",
    "print(classification_report(Y_val, predictions, labels=[1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3fZW0kPQnBM"
   },
   "source": [
    "We can already that there was a small improvement when removing unnecessary words. This means that the model can focus on more informative features.\n",
    "\n",
    "# 4. Word frequency\n",
    "\n",
    "Our bag-of-words model can already detect pretty well some of the easier classes. For example, Precision for digestive system diseases (class 1) is 0.7, which means that a prediction for class is going to be correct 70% of the time. \n",
    "\n",
    "The feature vectors now simply consist of counting the words of the vocabulary. Now what if we have a longer abstract and there are more occurrences of the words than in the training set? In this case, it would be beneficial to scale the frequencies based on text length. We can do that with the scikit-learn function `TfidfVectorizer`. Tf–idf is for “Term Frequency times Inverse Document Frequency”, meaning that the word count is divided by text length and words that occur in multiple documents are given a smaller weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96307,
     "status": "ok",
     "timestamp": 1621316349524,
     "user": {
      "displayName": "Santeri Rytky",
      "photoUrl": "",
      "userId": "00637237915246853555"
     },
     "user_tz": -180
    },
    "id": "fwQ7XKblkJMw",
    "outputId": "29668f78-0b18-4d29-8d9e-7406cdfee4f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.566\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.70      0.69       705\n",
      "           2       0.56      0.35      0.43       338\n",
      "           3       0.54      0.32      0.40       427\n",
      "           4       0.67      0.62      0.64       639\n",
      "           5       0.47      0.61      0.53      1084\n",
      "\n",
      "    accuracy                           0.57      3193\n",
      "   macro avg       0.59      0.52      0.54      3193\n",
      "weighted avg       0.58      0.57      0.56      3193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "X_train_vector = tfidf_vectorizer.transform(X_train)\n",
    "X_val_vector = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "# Train the classifier\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vector, Y_train)\n",
    "\n",
    "# Predict on validation data\n",
    "predictions = model.predict(X_val_vector)\n",
    "print(f'Accuracy: {accuracy_score(predictions, Y_val):.3f}')\n",
    "print(classification_report(Y_val, predictions, labels=[1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnYxqArcnF-P"
   },
   "source": [
    "We can see that the model is again working better. It is clearly important to account for changes in document length and account for commonly occurring words.\n",
    "\n",
    "# 5. Pipelines\n",
    "\n",
    "To conduct all of these steps in one go, we can combine the feature extraction and classification into a pipeline. This makes our job easier later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 102867,
     "status": "ok",
     "timestamp": 1621316356684,
     "user": {
      "displayName": "Santeri Rytky",
      "photoUrl": "",
      "userId": "00637237915246853555"
     },
     "user_tz": -180
    },
    "id": "DJNkjkjfpaLO",
    "outputId": "1d02347e-7500-458a-a1c8-94d00a7541fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.566\n"
     ]
    }
   ],
   "source": [
    "abstract_clf = Pipeline([('vectorizer', TfidfVectorizer(lowercase=True, stop_words='english')),\n",
    "                         ('classifier', LogisticRegression())])\n",
    "\n",
    "abstract_clf.fit(X_train, Y_train)\n",
    "predictions = abstract_clf.predict(X_val)\n",
    "print(f'Accuracy: {accuracy_score(predictions, Y_val):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmbRXNQkq8M_"
   },
   "source": [
    "We got the same result using the pipeline approach, which shows that we did not change anything other than compile the methods together.\n",
    "\n",
    "Let's now try to change a better classifier: Support vector machine (SVM). SVMs are generally considered very good for classifying text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2711,
     "status": "ok",
     "timestamp": 1621317329224,
     "user": {
      "displayName": "Santeri Rytky",
      "photoUrl": "",
      "userId": "00637237915246853555"
     },
     "user_tz": -180
    },
    "id": "Le54nCN3qX8e",
    "outputId": "41bc9fd9-868e-4198-a221-1eeb40aad830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.573\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.73      0.71       705\n",
      "           2       0.52      0.42      0.46       338\n",
      "           3       0.53      0.40      0.45       427\n",
      "           4       0.65      0.65      0.65       639\n",
      "           5       0.49      0.55      0.52      1084\n",
      "\n",
      "    accuracy                           0.57      3193\n",
      "   macro avg       0.57      0.55      0.56      3193\n",
      "weighted avg       0.57      0.57      0.57      3193\n",
      "\n",
      "        Predicted 1  Predicted 2  Predicted 3  Predicted 4  Predicted 5\n",
      "True 1          513           23           25           21          123\n",
      "True 2           40          141            3           10          144\n",
      "True 3           41            3          169           28          186\n",
      "True 4           16            9           32          416          166\n",
      "True 5          139           97           91          165          592\n"
     ]
    }
   ],
   "source": [
    "abstract_clf = Pipeline([('vectorizer', TfidfVectorizer(lowercase=True, stop_words='english')),\n",
    "                         ('classifier', SVC(kernel='linear', random_state=seed))])\n",
    "\n",
    "abstract_clf.fit(X_train, Y_train)\n",
    "predictions = abstract_clf.predict(X_val)\n",
    "\n",
    "# Predict on test for later\n",
    "predictions_svm = abstract_clf.predict(X_test)\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(predictions, Y_val):.3f}')\n",
    "print(classification_report(Y_val, predictions, labels=[1, 2, 3, 4, 5]))\n",
    "\n",
    "# Create confusion matrix\n",
    "conf = confusion_matrix(Y_val, predictions)\n",
    "true_labels = ['True 1', 'True 2', 'True 3', 'True 4', 'True 5']\n",
    "pred_labels = ['Predicted 1', 'Predicted 2', 'Predicted 3', 'Predicted 4', 'Predicted 5']\n",
    "print(pd.DataFrame(conf, index=true_labels,\n",
    "             columns=pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kqj0fJBL8e0"
   },
   "source": [
    "It seems that SVM was a better choice compared to logistic regression. However, there are many other design choices available, and we will discuss one option how to deal with them next.\n",
    "\n",
    "# 6. Weighted sampling\n",
    "\n",
    "Weighted sampling allows to account for the imbalance between the different classes. This can be achieved easily by passing the `class_weight='balanced'` argument. This means that the weights are inversely proportional to their frequency. Thus, underrepresented classes are accounted for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3468,
     "status": "ok",
     "timestamp": 1621317331135,
     "user": {
      "displayName": "Santeri Rytky",
      "photoUrl": "",
      "userId": "00637237915246853555"
     },
     "user_tz": -180
    },
    "id": "trCWp-ShASPB",
    "outputId": "29e72a71-e017-46d7-fecb-450cdf80aa41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.593\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.74      0.71       705\n",
      "           2       0.51      0.62      0.56       338\n",
      "           3       0.50      0.58      0.54       427\n",
      "           4       0.64      0.72      0.68       639\n",
      "           5       0.56      0.42      0.48      1084\n",
      "\n",
      "    accuracy                           0.59      3193\n",
      "   macro avg       0.58      0.62      0.59      3193\n",
      "weighted avg       0.59      0.59      0.59      3193\n",
      "\n",
      "        Predicted 1  Predicted 2  Predicted 3  Predicted 4  Predicted 5\n",
      "True 1          522           42           47           20           74\n",
      "True 2           37          211            9           12           69\n",
      "True 3           36           11          246           31          103\n",
      "True 4           17           14           42          462          104\n",
      "True 5          146          137          148          202          451\n"
     ]
    }
   ],
   "source": [
    "abstract_clf = Pipeline([('vectorizer', TfidfVectorizer(lowercase=True, stop_words='english')),\n",
    "                         ('classifier', SVC(kernel='linear', random_state=seed, class_weight='balanced'))])\n",
    "\n",
    "abstract_clf.fit(X_train, Y_train)\n",
    "predictions = abstract_clf.predict(X_val)\n",
    "\n",
    "# Predict on test for later\n",
    "predictions_bal = abstract_clf.predict(X_test)\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(predictions, Y_val):.3f}')\n",
    "print(classification_report(Y_val, predictions, labels=[1, 2, 3, 4, 5]))\n",
    "\n",
    "# Create confusion matrix\n",
    "conf = confusion_matrix(Y_val, predictions)\n",
    "true_labels = ['True 1', 'True 2', 'True 3', 'True 4', 'True 5']\n",
    "pred_labels = ['Predicted 1', 'Predicted 2', 'Predicted 3', 'Predicted 4', 'Predicted 5']\n",
    "print(pd.DataFrame(conf, index=true_labels,\n",
    "             columns=pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVGhln_K9PsY"
   },
   "source": [
    "We can immediately see from the classification report that the metrics are improving for classes 2 and 3. However, the most common class has now much worse performance.\n",
    "\n",
    "# 7. Hyperparameter and feature selection\n",
    "This time we optimize for the best set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "5F2fRaf0MSwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 64 candidates, totalling 64 fits\n",
      "[CV 1/1; 1/64] START classifier__C=0.01, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=-1\n",
      "[CV 1/1; 2/64] START classifier__C=0.01, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=100\n",
      "[CV 1/1; 3/64] START classifier__C=0.01, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=500\n",
      "[CV 1/1; 4/64] START classifier__C=0.01, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=1000\n",
      "[CV 1/1; 5/64] START classifier__C=0.01, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=-1\n",
      "[CV 1/1; 6/64] START classifier__C=0.01, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=100\n",
      "[CV 1/1; 7/64] START classifier__C=0.01, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=500\n",
      "[CV 1/1; 8/64] START classifier__C=0.01, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 2/64] END classifier__C=0.01, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=100;, score=0.106 total time=   5.2s\n",
      "[CV 1/1; 9/64] START classifier__C=0.01, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=-1\n",
      "[CV 1/1; 6/64] END classifier__C=0.01, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=100;, score=0.106 total time=   5.3s\n",
      "[CV 1/1; 10/64] START classifier__C=0.01, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 10/64] END classifier__C=0.01, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=100;, score=0.413 total time=   5.8s\n",
      "[CV 1/1; 11/64] START classifier__C=0.01, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=500\n",
      "[CV 1/1; 7/64] END classifier__C=0.01, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=500;, score=0.106 total time=  17.0s\n",
      "[CV 1/1; 12/64] START classifier__C=0.01, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=1000\n",
      "[CV 1/1; 3/64] END classifier__C=0.01, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=500;, score=0.106 total time=  17.2s\n",
      "[CV 1/1; 13/64] START classifier__C=0.01, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 8/64] END classifier__C=0.01, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=1000;, score=0.134 total time=  25.1s\n",
      "[CV 1/1; 14/64] START classifier__C=0.01, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=100\n",
      "[CV 1/1; 4/64] END classifier__C=0.01, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=1000;, score=0.191 total time=  27.4s\n",
      "[CV 1/1; 15/64] START classifier__C=0.01, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 11/64] END classifier__C=0.01, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=500;, score=0.445 total time=  20.5s\n",
      "[CV 1/1; 16/64] START classifier__C=0.01, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=1000\n",
      "[CV 1/1; 14/64] END classifier__C=0.01, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=100;, score=0.452 total time=   6.5s\n",
      "[CV 1/1; 17/64] START classifier__C=0.1, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 9/64] END classifier__C=0.01, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=-1;, score=0.339 total time=  34.8s\n",
      "[CV 1/1; 18/64] START classifier__C=0.1, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 1/64] END classifier__C=0.01, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=-1;, score=0.134 total time=  42.8s\n",
      "[CV 1/1; 19/64] START classifier__C=0.1, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 12/64] END classifier__C=0.01, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=1000;, score=0.492 total time=  27.6s\n",
      "[CV 1/1; 20/64] START classifier__C=0.1, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=1000\n",
      "[CV 1/1; 18/64] END classifier__C=0.1, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=100;, score=0.106 total time=   5.4s\n",
      "[CV 1/1; 21/64] START classifier__C=0.1, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=-1\n",
      "[CV 1/1; 5/64] END classifier__C=0.01, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=-1;, score=0.134 total time=  46.3s\n",
      "[CV 1/1; 22/64] START classifier__C=0.1, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=100\n",
      "[CV 1/1; 15/64] END classifier__C=0.01, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=500;, score=0.459 total time=  20.6s\n",
      "[CV 1/1; 23/64] START classifier__C=0.1, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 22/64] END classifier__C=0.1, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=100;, score=0.106 total time=   5.1s\n",
      "[CV 1/1; 24/64] START classifier__C=0.1, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 13/64] END classifier__C=0.01, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=-1;, score=0.339 total time=  39.5s\n",
      "[CV 1/1; 25/64] START classifier__C=0.1, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 19/64] END classifier__C=0.1, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=500;, score=0.110 total time=  17.9s\n",
      "[CV 1/1; 26/64] START classifier__C=0.1, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 16/64] END classifier__C=0.01, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=1000;, score=0.479 total time=  31.7s\n",
      "[CV 1/1; 27/64] START classifier__C=0.1, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 23/64] END classifier__C=0.1, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=500;, score=0.111 total time=  17.5s\n",
      "[CV 1/1; 28/64] START classifier__C=0.1, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=1000\n",
      "[CV 1/1; 26/64] END classifier__C=0.1, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=100;, score=0.420 total time=   5.7s\n",
      "[CV 1/1; 29/64] START classifier__C=0.1, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 20/64] END classifier__C=0.1, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=1000;, score=0.299 total time=  26.5s\n",
      "[CV 1/1; 30/64] START classifier__C=0.1, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=100\n",
      "[CV 1/1; 17/64] END classifier__C=0.1, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=-1;, score=0.571 total time=  41.9s\n",
      "[CV 1/1; 31/64] START classifier__C=0.1, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 30/64] END classifier__C=0.1, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=100;, score=0.448 total time=   5.6s\n",
      "[CV 1/1; 32/64] START classifier__C=0.1, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=1000\n",
      "[CV 1/1; 24/64] END classifier__C=0.1, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=1000;, score=0.300 total time=  26.5s\n",
      "[CV 1/1; 33/64] START classifier__C=1, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=-1\n",
      "[CV 1/1; 27/64] END classifier__C=0.1, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=500;, score=0.512 total time=  19.4s\n",
      "[CV 1/1; 34/64] START classifier__C=1, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 21/64] END classifier__C=0.1, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=-1;, score=0.574 total time=  41.7s\n",
      "[CV 1/1; 35/64] START classifier__C=1, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=500\n",
      "[CV 1/1; 34/64] END classifier__C=1, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=100;, score=0.222 total time=   5.3s\n",
      "[CV 1/1; 36/64] START classifier__C=1, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 31/64] END classifier__C=0.1, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=500;, score=0.515 total time=  19.2s\n",
      "[CV 1/1; 37/64] START classifier__C=1, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=-1\n",
      "[CV 1/1; 25/64] END classifier__C=0.1, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=-1;, score=0.469 total time=  37.0s\n",
      "[CV 1/1; 38/64] START classifier__C=1, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=100\n",
      "[CV 1/1; 28/64] END classifier__C=0.1, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=1000;, score=0.530 total time=  30.8s\n",
      "[CV 1/1; 39/64] START classifier__C=1, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 38/64] END classifier__C=1, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=100;, score=0.189 total time=   5.3s\n",
      "[CV 1/1; 40/64] START classifier__C=1, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 29/64] END classifier__C=0.1, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=-1;, score=0.470 total time=  35.9s\n",
      "[CV 1/1; 41/64] START classifier__C=1, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=-1\n",
      "[CV 1/1; 35/64] END classifier__C=1, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=500;, score=0.521 total time=  17.3s\n",
      "[CV 1/1; 42/64] START classifier__C=1, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 32/64] END classifier__C=0.1, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=1000;, score=0.528 total time=  30.3s\n",
      "[CV 1/1; 43/64] START classifier__C=1, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 33/64] END classifier__C=1, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=-1;, score=0.593 total time=  31.8s\n",
      "[CV 1/1; 44/64] START classifier__C=1, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=1000\n",
      "[CV 1/1; 42/64] END classifier__C=1, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=100;, score=0.392 total time=   5.5s\n",
      "[CV 1/1; 45/64] START classifier__C=1, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=-1\n",
      "[CV 1/1; 36/64] END classifier__C=1, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=1000;, score=0.565 total time=  25.2s\n",
      "[CV 1/1; 46/64] START classifier__C=1, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=100\n",
      "[CV 1/1; 39/64] END classifier__C=1, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=500;, score=0.521 total time=  17.3s\n",
      "[CV 1/1; 47/64] START classifier__C=1, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 46/64] END classifier__C=1, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=100;, score=0.457 total time=   5.7s\n",
      "[CV 1/1; 48/64] START classifier__C=1, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 40/64] END classifier__C=1, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=1000;, score=0.558 total time=  24.0s\n",
      "[CV 1/1; 49/64] START classifier__C=10, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=-1\n",
      "[CV 1/1; 37/64] END classifier__C=1, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=-1;, score=0.605 total time=  30.4s\n",
      "[CV 1/1; 50/64] START classifier__C=10, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 43/64] END classifier__C=1, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=500;, score=0.538 total time=  18.4s\n",
      "[CV 1/1; 51/64] START classifier__C=10, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=500\n",
      "[CV 1/1; 50/64] END classifier__C=10, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=100;, score=0.432 total time=   4.7s\n",
      "[CV 1/1; 52/64] START classifier__C=10, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 47/64] END classifier__C=1, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=500;, score=0.558 total time=  18.4s\n",
      "[CV 1/1; 53/64] START classifier__C=10, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=-1\n",
      "[CV 1/1; 41/64] END classifier__C=1, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=-1;, score=0.573 total time=  31.2s\n",
      "[CV 1/1; 54/64] START classifier__C=10, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 44/64] END classifier__C=1, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=1000;, score=0.578 total time=  26.3s\n",
      "[CV 1/1; 55/64] START classifier__C=10, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 54/64] END classifier__C=10, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=100;, score=0.423 total time=   4.8s\n",
      "[CV 1/1; 56/64] START classifier__C=10, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=1000\n",
      "[CV 1/1; 45/64] END classifier__C=1, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=-1;, score=0.592 total time=  28.4s\n",
      "[CV 1/1; 57/64] START classifier__C=10, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=-1\n",
      "[CV 1/1; 51/64] END classifier__C=10, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=500;, score=0.492 total time=  17.4s\n",
      "[CV 1/1; 58/64] START classifier__C=10, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 48/64] END classifier__C=1, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=1000;, score=0.592 total time=  27.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 59/64] START classifier__C=10, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 58/64] END classifier__C=10, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=100;, score=0.438 total time=   5.7s\n",
      "[CV 1/1; 60/64] START classifier__C=10, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=1000\n",
      "[CV 1/1; 52/64] END classifier__C=10, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=1000;, score=0.508 total time=  23.3s\n",
      "[CV 1/1; 61/64] START classifier__C=10, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=-1\n",
      "[CV 1/1; 55/64] END classifier__C=10, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=500;, score=0.487 total time=  17.7s\n",
      "[CV 1/1; 62/64] START classifier__C=10, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 49/64] END classifier__C=10, classifier__class_weight=balanced, classifier__kernel=linear, classifier__max_iter=-1;, score=0.516 total time=  34.0s\n",
      "[CV 1/1; 63/64] START classifier__C=10, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 62/64] END classifier__C=10, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=100;, score=0.444 total time=   4.9s\n",
      "[CV 1/1; 64/64] START classifier__C=10, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=1000\n",
      "[CV 1/1; 53/64] END classifier__C=10, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=-1;, score=0.502 total time=  27.0s\n",
      "[CV 1/1; 56/64] END classifier__C=10, classifier__class_weight=balanced, classifier__kernel=sigmoid, classifier__max_iter=1000;, score=0.498 total time=  22.9s\n",
      "[CV 1/1; 59/64] END classifier__C=10, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=500;, score=0.488 total time=  17.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 63/64] END classifier__C=10, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=500;, score=0.490 total time=  13.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hpham/miniconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 60/64] END classifier__C=10, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=1000;, score=0.519 total time=  21.9s\n",
      "[CV 1/1; 57/64] END classifier__C=10, classifier__class_weight=None, classifier__kernel=linear, classifier__max_iter=-1;, score=0.522 total time=  34.8s\n",
      "[CV 1/1; 61/64] END classifier__C=10, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=-1;, score=0.499 total time=  24.6s\n",
      "[CV 1/1; 64/64] END classifier__C=10, classifier__class_weight=None, classifier__kernel=sigmoid, classifier__max_iter=1000;, score=0.490 total time=  17.4s\n",
      "Best parameters: {'classifier__C': 1, 'classifier__class_weight': 'balanced', 'classifier__kernel': 'sigmoid', 'classifier__max_iter': -1}\n",
      "Best F1 score: 0.605\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "\n",
    "# Define the parameter grid for SVC\n",
    "param_dist = {\n",
    "    'classifier__kernel': ['linear', 'sigmoid'],\n",
    "    'classifier__C': [1e-2, 1e-1, 1, 10],\n",
    "    'classifier__max_iter': [-1, 100, 500, 1000],\n",
    "    'classifier__class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(lowercase=True, stop_words='english')),\n",
    "    ('classifier', SVC(random_state=seed))\n",
    "])\n",
    "\n",
    "# Build a predefined train/validation split using existing X_train/X_val\n",
    "X_train_val = X_train + X_val\n",
    "Y_train_val = Y_train + Y_val\n",
    "\n",
    "# -1 indicates training samples; 0 indicates validation fold\n",
    "test_fold = [-1] * len(X_train) + [0] * len(X_val)\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "# Randomized search with predefined split\n",
    "search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_dist,\n",
    "    scoring='accuracy',\n",
    "    cv=ps,\n",
    "    n_jobs=-1,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "search.fit(X_train_val, Y_train_val)\n",
    "print(\"Best parameters:\", search.best_params_)\n",
    "print(f'Best F1 score: {search.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tPCB6IWNuCk"
   },
   "source": [
    "The best set of parameters is stored in the `study` object. We can now use it to train the SGD classifier on the training set, and see the results on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2508,
     "status": "ok",
     "timestamp": 1621318139149,
     "user": {
      "displayName": "Santeri Rytky",
      "photoUrl": "",
      "userId": "00637237915246853555"
     },
     "user_tz": -180
    },
    "id": "di_bn6lmNsvt",
    "outputId": "194f4bef-2cfe-4cdd-8bd1-0a3bf77b1dc9"
   },
   "outputs": [],
   "source": [
    "# Use the best model for future\n",
    "abstract_clf = search.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "abstract_clf.fit(X_train, Y_train)\n",
    "predictions_opt = abstract_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtL8DT6ifmK8"
   },
   "source": [
    "It is time to make the final comparison to see which method performed best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 182874,
     "status": "ok",
     "timestamp": 1621317995604,
     "user": {
      "displayName": "Santeri Rytky",
      "photoUrl": "",
      "userId": "00637237915246853555"
     },
     "user_tz": -180
    },
    "id": "v0ZJWKotcjiD",
    "outputId": "c6599d79-141e-4e35-9a41-97ecbbed703f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline:\n",
      "Accuracy: 0.500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.63      0.63      1012\n",
      "           2       0.40      0.35      0.38       462\n",
      "           3       0.49      0.42      0.45       681\n",
      "           4       0.62      0.53      0.57      1091\n",
      "           5       0.39      0.46      0.42      1519\n",
      "\n",
      "    accuracy                           0.50      4765\n",
      "   macro avg       0.51      0.48      0.49      4765\n",
      "weighted avg       0.51      0.50      0.50      4765\n",
      "\n",
      "Confusion matrix: \n",
      "         Predicted 1  Predicted 2  Predicted 3  Predicted 4  Predicted 5\n",
      "True 1          642           48           47           36          239\n",
      "True 2           56          164           17           20          205\n",
      "True 3           76           10          288           49          258\n",
      "True 4           31           18           51          582          409\n",
      "True 5          213          167          184          250          705\n",
      "\n",
      "____________\n",
      "Support vector machine:\n",
      "Accuracy: 0.570\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.70      0.73      0.72      1012\n",
      "           2       0.49      0.40      0.44       462\n",
      "           3       0.56      0.41      0.47       681\n",
      "           4       0.70      0.63      0.66      1091\n",
      "           5       0.45      0.55      0.49      1519\n",
      "\n",
      "    accuracy                           0.57      4765\n",
      "   macro avg       0.58      0.54      0.56      4765\n",
      "weighted avg       0.58      0.57      0.57      4765\n",
      "\n",
      "Confusion matrix: \n",
      "         Predicted 1  Predicted 2  Predicted 3  Predicted 4  Predicted 5\n",
      "True 1          735           36           34           17          190\n",
      "True 2           41          186           11           10          214\n",
      "True 3           54            7          277           42          301\n",
      "True 4           26           14           39          688          324\n",
      "True 5          187          137          136          231          828\n",
      "\n",
      "____________\n",
      "Balanced SVM:\n",
      "Accuracy: 0.597\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.70      0.75      0.72      1012\n",
      "           2       0.47      0.60      0.53       462\n",
      "           3       0.52      0.60      0.56       681\n",
      "           4       0.70      0.73      0.71      1091\n",
      "           5       0.52      0.40      0.45      1519\n",
      "\n",
      "    accuracy                           0.60      4765\n",
      "   macro avg       0.58      0.62      0.59      4765\n",
      "weighted avg       0.59      0.60      0.59      4765\n",
      "\n",
      "Confusion matrix: \n",
      "         Predicted 1  Predicted 2  Predicted 3  Predicted 4  Predicted 5\n",
      "True 1          756           63           52           18          123\n",
      "True 2           44          276           19           14          109\n",
      "True 3           55           15          408           45          158\n",
      "True 4           23           28           67          797          176\n",
      "True 5          203          204          236          266          610\n",
      "\n",
      "____________\n",
      "Optimized SVM:\n",
      "Accuracy: 0.611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.76      0.74      1012\n",
      "           2       0.47      0.65      0.55       462\n",
      "           3       0.53      0.65      0.58       681\n",
      "           4       0.71      0.74      0.73      1091\n",
      "           5       0.55      0.39      0.46      1519\n",
      "\n",
      "    accuracy                           0.61      4765\n",
      "   macro avg       0.60      0.64      0.61      4765\n",
      "weighted avg       0.61      0.61      0.60      4765\n",
      "\n",
      "Confusion matrix: \n",
      "         Predicted 1  Predicted 2  Predicted 3  Predicted 4  Predicted 5\n",
      "True 1          766           68           54           17          107\n",
      "True 2           40          301           18           12           91\n",
      "True 3           44           17          441           42          137\n",
      "True 4           21           33           69          811          157\n",
      "True 5          200          215          248          263          593\n"
     ]
    }
   ],
   "source": [
    "print('Baseline:')\n",
    "print(f'Accuracy: {accuracy_score(predictions_bline, Y_test):.3f}')\n",
    "print(classification_report(Y_test, predictions_bline, labels=[1, 2, 3, 4, 5]))\n",
    "print('Confusion matrix: \\n', \n",
    "      pd.DataFrame(confusion_matrix(Y_test, predictions_bline), \n",
    "                   index=true_labels, columns=pred_labels))\n",
    "\n",
    "print('\\n____________\\nSupport vector machine:')\n",
    "print(f'Accuracy: {accuracy_score(predictions_svm, Y_test):.3f}')\n",
    "print(classification_report(Y_test, predictions_svm, labels=[1, 2, 3, 4, 5]))\n",
    "print('Confusion matrix: \\n', \n",
    "      pd.DataFrame(confusion_matrix(Y_test, predictions_svm), \n",
    "                   index=true_labels, columns=pred_labels))\n",
    "\n",
    "print('\\n____________\\nBalanced SVM:')\n",
    "print(f'Accuracy: {accuracy_score(predictions_bal, Y_test):.3f}')\n",
    "print(classification_report(Y_test, predictions_bal, labels=[1, 2, 3, 4, 5]))\n",
    "print('Confusion matrix: \\n', \n",
    "      pd.DataFrame(confusion_matrix(Y_test, predictions_bal), \n",
    "                   index=true_labels, columns=pred_labels))\n",
    "\n",
    "print('\\n____________\\nOptimized SVM:')\n",
    "print(f'Accuracy: {accuracy_score(predictions_opt, Y_test):.3f}')\n",
    "print(classification_report(Y_test, predictions_opt, labels=[1, 2, 3, 4, 5]))\n",
    "print('Confusion matrix: \\n', \n",
    "      pd.DataFrame(confusion_matrix(Y_test, predictions_opt), \n",
    "                   index=true_labels, columns=pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGyo0d5PgZA3"
   },
   "source": [
    "Seems that hyperparameter optimization yielded still a small improvement to the final results.\n",
    "\n",
    "Reminder for the classes:\n",
    "\n",
    "- 1 = Neoplasms (tumor)\n",
    "- 2 = Digestive system diseases\n",
    "- 3 = Nervous system diseases\n",
    "- 4 = Cardiovascular diseases\n",
    "- 5 = General pathological conditions\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "From the confusion matrices, we make some **conclusions on the results**:\n",
    "\n",
    "- Neoplasms and cardiovascular diseases are the easiest to classify (1 and 4), and likely have the most distinct vocabulary associated.\n",
    "\n",
    "- Many texts are classified as general pathological conditions (5). It is a frequent class, but also a logical option for uncertain cases.\n",
    "\n",
    "- Digestive and nervous system diseases (2 and 3) are less frequent and provide the worst results. However, balancing the class weights resolved this issue as seen in the results.\n",
    "\n",
    "**General conclusions**:\n",
    "\n",
    "- You now should have learned the basics for **multilabel classification** on **textual data**.\n",
    "\n",
    "- Since the text needs to be converted into numbers for the machine learning models, the medical abstracts have to be **vectorized**. **Bag-of-words** is the conventional approach, although other methods are also used such as **embedding**\n",
    "\n",
    "- **Pipelines** create an easy-to-use collection of the preprocessing and classification steps. Pipelines will be used in all upcoming tutorials.\n",
    "\n",
    "- **Class weights** allows accounting for **bias** in data (either binary or multi-class). Giving higher weight for the less frequent class allows the model to learn features for all individual classes. Always check for the **class frequencies** in your data!\n",
    "\n",
    "- When multiple possible hyperparameters need to be tuned, consider using **hyperparameter optimization**. In this case, one needs to be careful to avoid **overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8mqLxZsvHmG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMgC9tfEicKFU2KyDMqgVob",
   "collapsed_sections": [],
   "name": "Tutorial 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
